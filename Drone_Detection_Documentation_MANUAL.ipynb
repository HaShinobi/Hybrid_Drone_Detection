{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7971cd2b-9b9c-4dbe-ad24-7fee49ea873a",
   "metadata": {},
   "source": [
    "# Drone Detection Project Manual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1c8757-03bd-4388-bb22-597b4da7180e",
   "metadata": {},
   "source": [
    "The aim of this documentation is let users replicate the entire process of setup, training and running the Dolatro project. We tried to make this as user friendly as possible, please forgive us if there are any inconsistencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55414331-9b48-4a5a-9eb2-c4ea6fcdc6bd",
   "metadata": {},
   "source": [
    "### The following link explains the inital steps to set up a jetson nano 4GB version:\n",
    "https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit\n",
    "After successfully installing JetPack, plug the MicroSD card into the jetson Nano and power it on. Make sure to connect the two jumper pins (J48 Pin) on the Jetson before using an AC adapter. Read the guide above for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f715b8b7-5b68-4d64-ba6b-81e69632ca1b",
   "metadata": {},
   "source": [
    "## Ubuntu Setup\n",
    "\n",
    "Right after a fresh Ubuntu flash on your Jetson Nano, you’ll want to:\n",
    "\n",
    "Refresh and upgrade all system packages\n",
    "\n",
    "#### UPDATE SYSTEM\n",
    "sudo apt update\n",
    "sudo apt upgrade -y\n",
    "sudo reboot\n",
    "#### RUN THIS\n",
    "\n",
    "sudo apt install python3-pip -y\n",
    "\n",
    "### INSTALL CORE DEPENDENCIES\n",
    "Note that if you get errors using 'pip' use 'pip3' instead:\n",
    "\n",
    "pip3 install --upgrade pip\n",
    "pip3 install numpy pandas matplotlib\n",
    "pip3 install opencv-python\n",
    "pip3 install torch torchvision torchaudio\n",
    "python3 -m pip install --upgrade pip setuptools wheel\n",
    "\n",
    "#### INSTALL PYCUDA & CYTHON\n",
    "pip3 install cython==0.29.36\n",
    "export PATH=/usr/local/cuda-10.2/bin:${PATH:+:${PATH}}\n",
    "export LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64:$LD_LIBRARY_PATH\n",
    "python3 -m pip install pycuda --user\n",
    "\n",
    "\n",
    "\n",
    "#### CREATE VIRTUAL ENVIRONMENT WITH PRE_INSTALLED ROOT SITE PACKAGES\n",
    "python3 -m venv --system-site-packages ~/envs/pycuda_env_sys\n",
    "source ~/envs/pycuda_env_sys/bin/activate\n",
    "\n",
    "### INSTALL ACOUSTIC DETECTION PACKAGES FIRST!\n",
    "\n",
    "pip3 install sounddevice\n",
    "pip3 install pydub\n",
    "pip3 install pyusb\n",
    "\n",
    "For adafruit, do the following:\n",
    "\n",
    "sudo pip3 install -U \\\n",
    "adafruit-circuitpython-busdevice==5.1.2 \\\n",
    "adafruit-circuitpython-motor==3.3.5 \\\n",
    "adafruit-circuitpython-pca9685==3.4.1 \\\n",
    "adafruit-circuitpython-register==1.9.8 \\\n",
    "adafruit-circuitpython-servokit==1.3.8 \\\n",
    "Adafruit-Blinka==6.11.1 \\\n",
    "Adafruit-GPIO==1.0.3 \\\n",
    "Adafruit-MotorHAT==1.4.0 \\\n",
    "Adafruit-PlatformDetect==3.19.6 \\\n",
    "Adafruit-PureIO==1.1.9 \\\n",
    "Adafruit-SSD1306==1.6.2\n",
    "\n",
    "For librosa, do the following:\n",
    "\n",
    "Install Numba without dependencies:\n",
    "pip3 install numba==0.53\n",
    "\n",
    "Install LLVM 6.x:\n",
    "\n",
    "sudo apt update\n",
    "sudo apt install -y wget gnupg software-properties-common\n",
    "wget https://apt.llvm.org/llvm-snapshot.gpg.key\n",
    "sudo apt-key add llvm-snapshot.gpg.key\n",
    "\n",
    "sudo apt update\n",
    "sudo apt install -y wget gnupg software-properties-common\n",
    "wget https://apt.llvm.org/llvm-snapshot.gpg.key\n",
    "sudo apt-key add llvm-snapshot.gpg.key\n",
    "\n",
    "sudo add-apt-repository \"deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-9 main\"\n",
    "sudo apt update\n",
    "\n",
    "sudo apt install -y llvm-9 llvm-9-dev llvm-9-tools clang-9\n",
    "\n",
    "Now install llvmlite from source:\n",
    "\n",
    "sudo apt update\n",
    "sudo apt install -y python3-dev python3-pip build-essential cmake git libffi-dev\n",
    "\n",
    "git clone https://github.com/numba/llvmlite.git\n",
    "cd llvmlite\n",
    "git checkout v0.33.0\n",
    "\n",
    "\n",
    "export LLVM_CONFIG=/usr/bin/llvm-config-9\n",
    "python3 -m pip install .\n",
    "\n",
    "Finally, install compatoble librosa version:\n",
    "\n",
    "pip3 install librosa==0.8.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ffa99f-3cc0-4fa2-ad7a-2ed61e0e200b",
   "metadata": {},
   "source": [
    "## Python Setup:\n",
    "JetPack already comes preinstalled with Python 3.6. However, the system may still configured to install libraries on Python 2 version, which is not valid and should be changed. To check if the Python directory is correctly initiated, run the following code and observe the directories:\n",
    "\n",
    "\n",
    "Note: to run code using Ubuntu, simply create a text file on the text editor software that comes preinstalled in JetPack, write the code, and save it as a .py file. Alternatively, if your code will be short such as the code shown in the following cell, simply run \"python -m '(code)'\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "019cd659-d672-4415-a28e-5af51ffd2a14",
   "metadata": {},
   "source": [
    "./bashrc: #RUN THUS --> python -m 'import sys; print(\"\\n\".join(sys.path))'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e803a0d-67cb-4943-bb26-a9ee7c1d3f00",
   "metadata": {},
   "source": [
    "If you observe that the firstmost directory is not the correct one, being '/usr/local/lib/python3.6/dist‑packages'\n",
    "Run the following code to fix it:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5459482c-4f31-4649-bedf-d130170de274",
   "metadata": {},
   "source": [
    "./bashrc: #RUN THUS --> echo 'export PYTHONPATH=/usr/local/lib/python3.6/dist-packages:$PYTHONPATH' >> ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73a286a-a7f4-4c79-90c3-b50ef19025bb",
   "metadata": {},
   "source": [
    "# Visual Detection: Setting Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b852f85-7c7b-4b58-a972-4b3dd5a48753",
   "metadata": {},
   "source": [
    "###### Install Anaconda Navigator (Distribution Installer) for your operating system.\n",
    "###### https://www.anaconda.com/download/success\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02135a13-527a-4688-a69f-4b2d11d7bd97",
   "metadata": {},
   "source": [
    "###### Create a folder on which you will be working on. \n",
    "###### Make sure it is in the root of a storage directory to prevent future issues (example: \"C:/Project_Folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42266bdd-4cb4-49dd-99eb-3870ba0dfbf7",
   "metadata": {},
   "source": [
    "###### Search for \"anaconda prompt\" in Start. It will open a cmd like window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07e1db22-97db-42fa-aa7c-d0a63ffc33c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4178688365.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    conda create -n visualDetection\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Enter the following prompts (one by one and in order)\n",
    "\n",
    "#This will create the conda environment named visualDetection\n",
    "conda create -n visualDetection \n",
    "\n",
    "#Activate the environment after creation\n",
    "conda activate visualDetection\n",
    "\n",
    "#Set your directory to Project_Folder, so all you work will be saved on that folder\n",
    "cd \"path/to/your/folder\"   #REPLACE YOUR FOLDER PATH HERE\n",
    "\n",
    "#Then clone the yolov5 github repository\n",
    "git clone https://github.com/ultralytics/yolov5.git\n",
    "\n",
    "#change directory to the cloned yolov5 folder\n",
    "cd yolov5\n",
    "\n",
    "#Then run the requirements.txt and it will isntall all the required dependencies and libraries\n",
    "pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af58451-dadc-469c-9a1e-ac1fd32e9f5b",
   "metadata": {},
   "source": [
    "##### Datasets Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9561046e-2363-4cfa-82a0-e4361e501a97",
   "metadata": {},
   "source": [
    "###### Now we will get the datasets for our projects \n",
    "###### Download the First Dataset from https://drive.google.com/file/d/1NPYaop35ocVTYWHOYQQHn8YHsM9jmLGr/view\n",
    "###### Extract it in a new folder called \"Datasets\" inside the yolov5 folder directory\n",
    "###### We will first run a script to remove the infrared video samples first and then use the remaining rgb video sample to create images and labels, valid to YOLO training format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb578d8f-573e-404d-a2aa-2b4eb621246e",
   "metadata": {},
   "source": [
    "We will need to then convert the collected rgb video files to YOLO training format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfccbaa-8709-46a5-8cf3-618e26aa800b",
   "metadata": {},
   "source": [
    "# Acoustic Detection Setup:\n",
    "\n",
    "To perform Acoustic Detection, we need to perform the following:\n",
    "\n",
    "- Find a suitable dataset\n",
    "- Preprocess the dataset\n",
    "- Extract MFCCs\n",
    "- Train and Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3558d5f8-eefc-4eff-b741-1b48b2f1ee3e",
   "metadata": {},
   "source": [
    "## Step 1: Find a Suitable Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc9b9c2-4993-42cc-ac3f-eda4d2f59a9c",
   "metadata": {},
   "source": [
    "The dataset used for Acoustic Detection can be found form the following link: https://huggingface.co/datasets/geronimobasso/drone-audio-detection-samples, and it is claimed by the owner to be the largest dataset on the internet for drone audio, consisting of nearly 7 GB of files. We decided to select five parquets (~ 23,000 audio files) split equally in half for 'No drone' and 'Drone' data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab8d6d2-a6e4-42b3-9ea7-70343817c54c",
   "metadata": {},
   "source": [
    "## Step 2: Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d523e8e4-bb34-420e-aa7e-a94d6c8916f3",
   "metadata": {},
   "source": [
    "After downloading the dataset, the files will be grouped in parquet zip folders and will need a python script to be extracted and labeled correctly. Be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e89d01-1727-47b6-b190-19a5c1b3b6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import base64\n",
    "from glob import glob\n",
    "\n",
    "def decode_audio_field(field):\n",
    "    if isinstance(field, (bytes, bytearray)):\n",
    "        return bytes(field)\n",
    "    if isinstance(field, str):\n",
    "        try:\n",
    "            return base64.b64decode(field)\n",
    "        except Exception:\n",
    "            arr = json.loads(field)\n",
    "            return bytes(arr)\n",
    "    if isinstance(field, list):\n",
    "        return bytes(field)\n",
    "    raise ValueError(f\"Unsupported audio field type: {type(field)}\")\n",
    "\n",
    "def process_parquet_file(parquet_path, yes_dir, no_dir, audio_column=\"audio_bytes\", label_column='path'):\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    for idx, row in df.iterrows():\n",
    "        audio_bytes = decode_audio_field(row[audio_column])\n",
    "        \n",
    "        label = str(row[label_column]).strip().upper()\n",
    "        output_dir = no_dir if 'no-drone' in label else yes_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        base_name = os.path.splitext(os.path.basename(parquet_path))[0]\n",
    "        filename = f\"{base_name}_{idx}.wav\"\n",
    "        \n",
    "        out_path = os.path.join(output_dir, filename)\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            f.write(audio_bytes)\n",
    "\n",
    "def main(parquet_dir, yes_dir, no_dir, audio_column=\"bytes\", label_column=\"path\"):\n",
    "    parquet_files = glob(os.path.join(parquet_dir, \"*.parquet\"))\n",
    "    for pq in parquet_files:\n",
    "        process_parquet_file(pq, yes_dir, no_dir, audio_column, label_column)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description=\"Decode audio from Parquet files and save as WAV by label\")\n",
    "    parser.add_argument(\"parquet_dir\", help=\"Directory containing .parquet files\")\n",
    "    parser.add_argument(\"yes_dir\", help=\"Output directory for YES DRONE WAVs\")\n",
    "    parser.add_argument(\"no_dir\", help=\"Output directory for NO DRONE WAVs\")\n",
    "    parser.add_argument(\"--audio_column\", default=\"bytes\", help=\"Name of the audio bytes column in the Parquet files\")\n",
    "    parser.add_argument(\"--label_column\", default=\"path\", help=\"Name of the label column (YES_DRONE / NO_DRONE)\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Ensure output directories exist\n",
    "    os.makedirs(args.yes_dir, exist_ok=True)\n",
    "    os.makedirs(args.no_dir, exist_ok=True)\n",
    "    \n",
    "    main(args.parquet_dir, args.yes_dir, args.no_dir, args.audio_column, args.label_column)\n",
    "\n",
    "# Usage:\n",
    "# Save this script as `decode_wav.py`, then run:\n",
    "# python decode_wav.py /path/to/parquets /path/to/YES_DRONE /path/to/NO_DRONE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301b67b3-6d2a-4dc6-b887-d6390de21d87",
   "metadata": {},
   "source": [
    "## Step 3: Extract MFCCs and Set Up Data\n",
    "\n",
    "To extract MFCCs, follow these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e89a2de-f132-4001-a26d-64d262f0cd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow==2.4.1 (from versions: 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0)\n",
      "ERROR: No matching distribution found for tensorflow==2.4.1\n"
     ]
    }
   ],
   "source": [
    "# Install Dependencies\n",
    "\n",
    "!pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 tensorflow-io matplotlib\n",
    "\n",
    "# Build Dataloading Function:\n",
    "def load_wav_16k_mono(filename):\n",
    "    # Load encoded wav file\n",
    "    file_contents = tf.io.read_file(filename)\n",
    "    # Decode wav (tensors by channels) \n",
    "    wav, sample_rate = tf.audio.decode_wav(file_contents, desired_channels=1)\n",
    "    # Removes trailing axis\n",
    "    wav = tf.squeeze(wav, axis=-1)\n",
    "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
    "    # Goes from 44100Hz to 16000hz - amplitude of the audio signal\n",
    "    return wav\n",
    "\n",
    "\n",
    "# Concatenate Dataset:\n",
    "POS = os.path.join('path-to-pos-dir')\n",
    "NEG = os.path.join('path-to-neg-dir')\n",
    "pos = tf.data.Dataset.list_files(POS+'\\*.wav')\n",
    "neg = tf.data.Dataset.list_files(NEG+'\\*.wav')\n",
    "positives = tf.data.Dataset.zip((pos, tf.data.Dataset.from_tensor_slices(tf.ones(len(pos)))))\n",
    "negatives = tf.data.Dataset.zip((neg, tf.data.Dataset.from_tensor_slices(tf.zeros(len(neg)))))\n",
    "data = positives.concatenate(negatives)\n",
    "\n",
    "# Build preprocessing Function\n",
    "def preprocess(file_path, label): \n",
    "    wav = load_wav_16k_mono(file_path)\n",
    "    wav = wav[:'rate-of-audio-dataset']\n",
    "    zero_padding = tf.zeros(['rate-of-audio-dataset'] - tf.shape(wav), dtype=tf.float32)\n",
    "    wav = tf.concat([zero_padding, wav],0)\n",
    "    spectrogram = tf.signal.stft(wav, frame_length=320, frame_step=32)\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "    spectrogram = tf.expand_dims(spectrogram, axis=2)\n",
    "    print(spectrogram)\n",
    "    return spectrogram, label\n",
    "\n",
    "# Create Testing and Training Partitions\n",
    "data = data.map(preprocess)\n",
    "data = data.cache()\n",
    "data = data.shuffle('length-of-audio-dataset')\n",
    "data = data.batch(16).map(lambda x, y: (tf.ensure_shape(x, (None, 'SHAPE X', 'SHAPE Y', 'SHAPE Z')), y))  \n",
    "data = data.prefetch(8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b1b502-8b37-4239-9dae-dde7be514361",
   "metadata": {},
   "source": [
    "## Step 4: Training the Model\n",
    "\n",
    "Building the model may differ based on hardware and dataset size. One must undergo some trial and error before using the most optimal model. The model we have used for our dataset can be coded by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a0d6322-f319-4088-a326-81c26a611046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">241</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_9                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">241</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_10               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_11               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m241\u001b[0m, \u001b[38;5;34m257\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m320\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_9                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m241\u001b[0m, \u001b[38;5;34m257\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_9 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m18,496\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_10               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_10 (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m73,856\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_11               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_11 (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m16,512\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m129\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">110,209</span> (430.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m110,209\u001b[0m (430.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,761</span> (428.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m109,761\u001b[0m (428.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "\n",
    "input_shape = (241, 257, 1)\n",
    "\n",
    "model = Sequential([\n",
    "    # Conv Block 1\n",
    "    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Conv Block 2\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Conv Block 3\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Global Pooling instead of Flatten\n",
    "    GlobalAveragePooling2D(),\n",
    "\n",
    "    # Dense Layers\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),  # Helps prevent overfitting\n",
    "\n",
    "    # Output Layer\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "model.compile('Adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.Recall(),tf.keras.metrics.Precision()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b9901a-15da-49ea-bf19-8cd047604a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now, just fit and train the model\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "earlystop_cb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.001,\n",
    "    patience=1,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "hist = model.fit(\n",
    "    train,\n",
    "    epochs=50,\n",
    "    validation_data=test,\n",
    "    callbacks=[earlystop_cb]\n",
    ")\n",
    "\n",
    "model.save('model_name.keras')\n",
    "\n",
    "# Our model is ready and saved!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ffb30c-70a5-4a77-82f5-2e65fbd93418",
   "metadata": {},
   "source": [
    "## Step 5: Convert from .keras to ONNX\n",
    "\n",
    "After finishing up with our model, we need to convert it to ONNX:\n",
    "(Do this step on a Google Colab file!)\n",
    "\n",
    "## Install Dependencies:\n",
    "\n",
    "pip3 install keras tf2onnx keras tensorflow\n",
    "\n",
    "## RUN THIS\n",
    "\n",
    "import keras\n",
    "import tf2onnx\n",
    "import tensorflow as tf\n",
    "\n",
    "model = keras.models.load_model(\"model_name.keras\")\n",
    "spec = [tf.TensorSpec(shape=(None, 491, 257, 1), dtype=tf.float32, name='input')]\n",
    "model.output_names=['output']\n",
    "model_proto, _ = tf2onnx.convert.from_keras(\n",
    "    model,\n",
    "    input_signature=spec,\n",
    "    opset=11,\n",
    "    output_path=\"the_final_model3.onnx\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2bf019-0e3f-491c-bd36-dc6b020dc6f2",
   "metadata": {},
   "source": [
    "## STEP 6: Convert from ONNX to TRT\n",
    "\n",
    "On the Jetson Nano, do this:\n",
    "\n",
    "trtexec --onnx=model.onnx --saveEngine=model_fp32.engine --workspace=2048\n",
    "\n",
    "### Please make sure all files are accessed through their correct respective directories!\n",
    "### Congratulations, you should have the final model!\n",
    "\n",
    "## Now let's set up the ReSpeaker:\n",
    "\n",
    "Connect the respaker to the jetson\n",
    "\n",
    "sudo apt-get update\n",
    "sudo pip install pyusb click\n",
    "git clone https://github.com/respeaker/usb_4_mic_array.git\n",
    "cd usb_4_mic_array\n",
    "sudo python3 dfu.py --download 6_channels_firmware.bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019a8174-47b0-455b-a33a-58772af71d85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
